{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ETL: Amazon Products to PostgreSQL\n",
    "\n",
    "This notebook loads Amazon Electronics metadata directly from `.gz` files into PostgreSQL.\n",
    "\n",
    "**Supports:**\n",
    "- `.jsonl.gz` - compressed files (reads directly, no unpacking needed)\n",
    "- `.jsonl` - plain JSONL files\n",
    "\n",
    "**Prerequisites:**\n",
    "- PostgreSQL running via docker-compose (`make run-docker-compose`)\n",
    "- Data file: `meta_Electronics.jsonl.gz` (or preprocessed version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the path to your data file. Supports both `.jsonl` and `.jsonl.gz` formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    \"host\": \"0.0.0.0\",\n",
    "    \"port\": os.getenv(\"POSTGRES_PORT\", \"5432\"),\n",
    "    \"dbname\": os.getenv(\"POSTGRES_DB\", \"amazon_products\"),\n",
    "    \"user\": os.getenv(\"POSTGRES_USER\", \"bootcamp\"),\n",
    "    \"password\": os.getenv(\"POSTGRES_PASSWORD\", \"bootcamp\"),\n",
    "}\n",
    "\n",
    "# === CONFIGURE YOUR DATA FILE HERE ===\n",
    "# Option 1: Raw .gz file (full dataset ~2.7M items) - reads directly from compressed\n",
    "# DATA_FILE = \"../../data/meta_Electronics.jsonl.gz\"\n",
    "\n",
    "# Option 2: Preprocessed sample (1000 items)\n",
    "DATA_FILE = \"../../data/meta_Electronics_2022_2023_with_category_ratings_100_sample_1000.jsonl.gz\"\n",
    "\n",
    "# Filtering options (set to None to load all)\n",
    "MIN_RATING_COUNT = 100  # Only products with 100+ ratings (set None to disable)\n",
    "MIN_YEAR = 2022         # Only products from 2022+ (set None to disable)\n",
    "MAX_ROWS = None         # Limit rows (set None for all, e.g., 10000 for testing)\n",
    "\n",
    "print(f\"Data file: {DATA_FILE}\")\n",
    "print(f\"Filters: min_ratings={MIN_RATING_COUNT}, min_year={MIN_YEAR}, max_rows={MAX_ROWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## Connect to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a261c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT version();\")\n",
    "print(f\"Connected to: {cursor.fetchone()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-header",
   "metadata": {},
   "source": [
    "## Create Products Table\n",
    "\n",
    "Schema designed for SQL filtering in hybrid queries (price, rating, category filters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_TABLE_SQL = \"\"\"\n",
    "DROP TABLE IF EXISTS products CASCADE;\n",
    "\n",
    "CREATE TABLE products (\n",
    "    asin VARCHAR(20) PRIMARY KEY,\n",
    "    parent_asin VARCHAR(20),\n",
    "    title TEXT,\n",
    "    price DECIMAL(10, 2),\n",
    "    average_rating DECIMAL(3, 2),\n",
    "    rating_number INTEGER,\n",
    "    main_category VARCHAR(100),\n",
    "    store VARCHAR(255),\n",
    "    description TEXT,\n",
    "    features JSONB,\n",
    "    created_at TIMESTAMP DEFAULT NOW()\n",
    ");\n",
    "\n",
    "-- Indexes for common query patterns\n",
    "CREATE INDEX idx_products_rating ON products(average_rating);\n",
    "CREATE INDEX idx_products_price ON products(price);\n",
    "CREATE INDEX idx_products_category ON products(main_category);\n",
    "CREATE INDEX idx_products_rating_number ON products(rating_number);\n",
    "CREATE INDEX idx_products_parent_asin ON products(parent_asin);\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(CREATE_TABLE_SQL)\n",
    "print(\"Table 'products' created with indexes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Data from JSONL / JSONL.GZ\n",
    "\n",
    "Supports both compressed and uncompressed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filepath):\n",
    "    \"\"\"Open file, auto-detecting .gz compression.\"\"\"\n",
    "    if str(filepath).endswith('.gz'):\n",
    "        return gzip.open(filepath, 'rt', encoding='utf-8')\n",
    "    return open(filepath, 'r', encoding='utf-8')\n",
    "\n",
    "\n",
    "def parse_price(price_val):\n",
    "    \"\"\"Extract numeric price from various formats.\"\"\"\n",
    "    if price_val is None:\n",
    "        return None\n",
    "    if isinstance(price_val, (int, float)):\n",
    "        return float(price_val)\n",
    "    if isinstance(price_val, str):\n",
    "        cleaned = price_val.replace('$', '').replace(',', '').strip()\n",
    "        try:\n",
    "            return float(cleaned)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_year(data):\n",
    "    \"\"\"Extract year from 'Date First Available' in details.\"\"\"\n",
    "    try:\n",
    "        details = data.get('details', {})\n",
    "        date_str = details.get('Date First Available', '')\n",
    "        if date_str:\n",
    "            return int(date_str[-4:])\n",
    "    except (ValueError, TypeError, KeyError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def should_include(data, min_rating_count=None, min_year=None):\n",
    "    \"\"\"Check if record passes filters.\"\"\"\n",
    "    if min_rating_count and (data.get('rating_number') or 0) < min_rating_count:\n",
    "        return False\n",
    "    if min_year:\n",
    "        year = extract_year(data)\n",
    "        if year and year < min_year:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_to_postgres(\n",
    "    filepath, \n",
    "    cursor, \n",
    "    batch_size=500,\n",
    "    min_rating_count=None,\n",
    "    min_year=None,\n",
    "    max_rows=None\n",
    "):\n",
    "    \"\"\"Load JSONL/JSONL.GZ file into products table with filtering.\"\"\"\n",
    "    \n",
    "    INSERT_SQL = \"\"\"\n",
    "        INSERT INTO products (\n",
    "            asin, parent_asin, title, price, average_rating, \n",
    "            rating_number, main_category, store, description, features\n",
    "        ) VALUES %s\n",
    "        ON CONFLICT (asin) DO UPDATE SET\n",
    "            title = EXCLUDED.title,\n",
    "            price = EXCLUDED.price,\n",
    "            average_rating = EXCLUDED.average_rating,\n",
    "            rating_number = EXCLUDED.rating_number\n",
    "    \"\"\"\n",
    "    \n",
    "    batch = []\n",
    "    total_loaded = 0\n",
    "    total_processed = 0\n",
    "    total_skipped = 0\n",
    "    \n",
    "    with open_file(filepath) as f:\n",
    "        for line in f:\n",
    "            total_processed += 1\n",
    "            \n",
    "            if total_processed % 100000 == 0:\n",
    "                print(f\"Processed {total_processed:,} lines, loaded {total_loaded:,}, skipped {total_skipped:,}\")\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "            except json.JSONDecodeError:\n",
    "                total_skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Apply filters\n",
    "            if not should_include(data, min_rating_count, min_year):\n",
    "                total_skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Use parent_asin as fallback if asin is missing\n",
    "            asin = data.get('asin') or data.get('parent_asin')\n",
    "            if not asin:\n",
    "                total_skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Extract description\n",
    "            description = None\n",
    "            if data.get('description'):\n",
    "                if isinstance(data['description'], list):\n",
    "                    description = ' '.join(str(d) for d in data['description'])\n",
    "                else:\n",
    "                    description = str(data['description'])\n",
    "            \n",
    "            # Prepare row\n",
    "            row = (\n",
    "                asin,\n",
    "                data.get('parent_asin'),\n",
    "                data.get('title'),\n",
    "                parse_price(data.get('price')),\n",
    "                data.get('average_rating'),\n",
    "                data.get('rating_number'),\n",
    "                data.get('main_category'),\n",
    "                data.get('store'),\n",
    "                description,\n",
    "                json.dumps(data.get('features', [])),\n",
    "            )\n",
    "            batch.append(row)\n",
    "            \n",
    "            if len(batch) >= batch_size:\n",
    "                execute_values(cursor, INSERT_SQL, batch)\n",
    "                total_loaded += len(batch)\n",
    "                batch = []\n",
    "                \n",
    "                if max_rows and total_loaded >= max_rows:\n",
    "                    print(f\"Reached max_rows limit: {max_rows}\")\n",
    "                    break\n",
    "    \n",
    "    # Insert remaining\n",
    "    if batch:\n",
    "        execute_values(cursor, INSERT_SQL, batch)\n",
    "        total_loaded += len(batch)\n",
    "    \n",
    "    return total_loaded, total_processed, total_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading from: {DATA_FILE}\")\n",
    "print(f\"File exists: {Path(DATA_FILE).exists()}\")\n",
    "print()\n",
    "\n",
    "loaded, processed, skipped = load_to_postgres(\n",
    "    DATA_FILE, \n",
    "    cursor,\n",
    "    min_rating_count=MIN_RATING_COUNT,\n",
    "    min_year=MIN_YEAR,\n",
    "    max_rows=MAX_ROWS\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f\"=== ETL Complete ===\")\n",
    "print(f\"Total processed: {processed:,}\")\n",
    "print(f\"Total loaded: {loaded:,}\")\n",
    "print(f\"Total skipped: {skipped:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "## Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT COUNT(*) FROM products;\")\n",
    "print(f\"Total products in database: {cursor.fetchone()[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    SELECT asin, title, price, average_rating, main_category \n",
    "    FROM products \n",
    "    LIMIT 5;\n",
    "\"\"\")\n",
    "\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"ASIN: {row[0]}\")\n",
    "    title = row[1][:60] + '...' if row[1] and len(row[1]) > 60 else row[1]\n",
    "    print(f\"  Title: {title}\")\n",
    "    print(f\"  Price: ${row[2]}, Rating: {row[3]}, Category: {row[4]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-header",
   "metadata": {},
   "source": [
    "## Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    SELECT main_category, COUNT(*) as cnt \n",
    "    FROM products \n",
    "    GROUP BY main_category \n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 20;\n",
    "\"\"\")\n",
    "\n",
    "print(\"Products by Category (top 20):\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row[0]}: {row[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        MIN(price) as min_price,\n",
    "        MAX(price) as max_price,\n",
    "        AVG(price) as avg_price,\n",
    "        COUNT(*) FILTER (WHERE price IS NOT NULL) as with_price\n",
    "    FROM products;\n",
    "\"\"\")\n",
    "\n",
    "row = cursor.fetchone()\n",
    "print(f\"Price Statistics:\")\n",
    "print(f\"  Min: ${row[0]}, Max: ${row[1]}, Avg: ${row[2]:.2f}\" if row[2] else \"  No price data\")\n",
    "print(f\"  Products with price: {row[3]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        MIN(average_rating) as min_rating,\n",
    "        MAX(average_rating) as max_rating,\n",
    "        AVG(average_rating) as avg_rating,\n",
    "        MIN(rating_number) as min_reviews,\n",
    "        MAX(rating_number) as max_reviews\n",
    "    FROM products;\n",
    "\"\"\")\n",
    "\n",
    "row = cursor.fetchone()\n",
    "print(f\"Rating Statistics:\")\n",
    "print(f\"  Rating range: {row[0]} - {row[1]}, Avg: {row[2]:.2f}\")\n",
    "print(f\"  Review count range: {row[3]:,} - {row[4]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Database connection closed.\")\n",
    "print()\n",
    "print(\"You can now delete the source .gz file if needed - data is safely in PostgreSQL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25b4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
